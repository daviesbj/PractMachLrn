---
title: Practical Machine Learning Study Report
author: "Brian Davies"
date: "04 November 2014"
output: html_document
---

# Data Acquisition, cleanup, split

```{r housekeeping}
# rm(list=ls())
set.seed(19610910)
library(caret); library(ggplot2); library(klaR); library(MASS)
```
```{r download and peek at two datasets, eval = FALSE}
download.file( 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
               destfile = 'pml-training.csv', method = 'curl' )
download.file( 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
               destfile = 'pml-testing.csv', method = 'curl' )
```
```{r load and partition data, cache = TRUE}
inputSetFull <- read.csv('pml-training.csv', stringsAsFactors = FALSE )
testSetFull <- read.csv('pml-testing.csv', stringsAsFactors = FALSE )
availVars <- colnames(testSetFull)[(sapply(colnames(testSetFull),function(cn){sum(is.na(testSetFull[,cn]))}) == 0)][1:59]
inputSet <- inputSetFull[,c(availVars,'classe')]
inputSet$classe <- factor(inputSet$classe)
sum(is.na(inputSet))
trainFrac <- 0.75
inTrain <- createDataPartition( y=inputSet$classe, p = trainFrac, list = FALSE )
trnSet <- inputSet[ inTrain, ]
valSet <- inputSet[ -inTrain, ]
inCols <- colnames(trnSet)[grep('_belt|_arm|_dumb|_forearm',colnames(trnSet))]
trnIO <- data.frame( trnSet[,inCols], classe = trnSet$classe, row.names = NULL )
valIO <- data.frame( valSet[,inCols], classe = valSet$classe, row.names = NULL )
```

I have now divided my data into a training set
(`r sprintf( '%d%s', trainFrac * 100, '%' )`)
and a validation set
(`r sprintf( '%d%s', (1 - trainFrac) * 100, '%' )`)
consisting of all numerical-valued inputs and the `classe` factor. It seems pretty
obvious that I shouldn't use the `user_name` variable or any of the timestamps
as predictors. Also, all the summary-statistic parms are only quoted once per
timeslice, and aren't available in the testing dataset. That leaves us with
52 predictor variables -- quite a lot, but not crazy. It will probably be worth
taking principal components at some point.

# Classifier training and validation

For industrial-trength use, I would hope for a prediction accuracy of __95%__
meaning that I would expect to identify `classe` correctly 95% of the time when I
apply my trained predictor to the validation set `valIO`. For the purposes of
this study, I will be happy with 90% accuracy.

It seems that the caret `train()` function has a mechanism to search over a
multidimensional grid of training options to search for the best overall
predictor. At first, at least, I'm not going to try that (until I know that
I can control it).

## Naive Bayes

Even for this one I found `train()` a bit too automatic. I wasn't able to
find a way to get it to just train on the entire dataset -- I got a nasty
error message when I tried the settings in the example in the Week 2 lecture. I
had to do it with cross-validation and it insisted on trying with and
without kernel smoothing, which I couldn't figure out how to turn off; I just
wanted to use straight Gaussian regression, and kernel smoothing took ages.
I still use `valIO` for my own validation pass at the end; `train()` selected
the kernel-smoothed option, probably not surprising.

```{r train_and_test_Naive_Bayes, cache = TRUE, warning = FALSE }
modNB <- train( classe~. , data = trnIO, method = "nb",
                trControl = trainControl( method = 'cv', number = 3 ) )
modNB$bestTune
predNB <- predict( modNB, valIO )
confMatNB <- confusionMatrix( predNB, valIO$classe )
confMatNB$table
confMatNB$overall
```

__Accuracy of the Naive Bayes Classifier =
`r sprintf( '%.1f%s', confMatNB$overall['Accuracy'] * 100, '%' )` __

## Classification tree using all inputs

Now that I've got the framework working, I'm going to try to use a tree
classifier like in the lectures

```{r train_and_test_Tree, cache = TRUE, warning = FALSE }
modTree <- train( classe~. , data = trnIO, method = "rpart" )
predTree <- predict( modTree, valIO )
confMatTree <- confusionMatrix( predTree, valIO$classe )
confMatTree$table
confMatTree$overall
```

__Accuracy of the First Tree Classifier =
`r sprintf( '%.1f%s', confMatTree$overall['Accuracy'] * 100, '%' )` __

## Classification tree using principal components of inputs

That last one really wasn't very good. Let's try another tree, but using
the principal components of the inputs.

```{r train_and_test_PCA_Tree, cache = TRUE, warning = FALSE }
modTreePCA <- train( classe~. , data = trnIO, method = "rpart",
                     preProcess = c( 'center', 'scale', 'pca' ) )
predTreePCA <- predict( modTreePCA, valIO )
confMatTreePCA <- confusionMatrix( predTreePCA, valIO$classe )
confMatTreePCA$table
confMatTreePCA$overall
```

__Accuracy of the PCA Tree Classifier =
`r sprintf( '%.1f%s', confMatTreePCA$overall['Accuracy'] * 100, '%' )` __

## Random Forest

Let's bring out the big guns!

```{r train_and_test_Random_Forest, cache = TRUE, warning = FALSE }
modRF <- train( classe~. , data = trnIO, method = "rf", prox = TRUE )
predRF <- predict( modRF, valIO )
confMatRF <- confusionMatrix( predRF, valIO$classe )
confMatRF$table
confMatRF$overall
```

__Accuracy of the Random Forest Classifier =
`r sprintf( '%.1f%s', confMatRF$overall['Accuracy'] * 100, '%' )` __

That's __amazing__ -- better save a copy!

```{r save_Random_Forest_Classifier, cache = TRUE }
save( modRF, file = 'RandomForestClassifier.Rdata' )
```

# Conclusion -- choice of classifier

Here's a summary of the results

Classifier    | Out of Sample Accuracy (%)
--------------|---------------------------
Naive Bayes   | `r sprintf( '%.1f%s', confMatNB$overall['Accuracy'] * 100, '%' )`
Basic Tree    | `r sprintf( '%.1f%s', confMatTree$overall['Accuracy'] * 100, '%' )`
Tree with PCA | `r sprintf( '%.1f%s', confMatTreePCA$overall['Accuracy'] * 100, '%' )`
Random Forest | `r sprintf( '%.1f%s', confMatRF$overall['Accuracy'] * 100, '%' )`

So the best one is obviously the Random Forest -- that's what I recommend to use
